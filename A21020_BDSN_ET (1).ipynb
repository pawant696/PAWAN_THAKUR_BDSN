{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A21020_BDSN_ET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **BDSN END TERM ASSIGNMENT**"
      ],
      "metadata": {
        "id": "AZdu0qg43e4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the latest Version of Spark\n"
      ],
      "metadata": {
        "id": "eghjBvvt5ApY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdHevNNa3R57",
        "outputId": "20b81624-42d5-496a-e3d8-a8de44066004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "^C\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt update > /dev/null\n",
        "!apt install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get latest and correct version of Spark\n",
        "#\n",
        "# if the current version of Spark is not used, there may be errors\n",
        "# check here for current versions http://apache.osuosl.org/spark\n",
        "#\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.2.2/spark-2.2.2-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "#!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#!pip install -q findspark\n",
        "!pip install -q pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU4jmjXb31qA",
        "outputId": "edb2a6d4-bcf6-4173-ea69-97aa79f2be6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing SparkSession from Pyspark and building SparkSession\n"
      ],
      "metadata": {
        "id": "rqFUXIb75Gs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# note UI port switched from default 4040 to 4050 to avoid clash with ngrok\n",
        "spark = SparkSession.builder.master(\"local[*]\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "ugJYMrel3-mL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing KMeans from ml.clustering Module of Pyspark"
      ],
      "metadata": {
        "id": "dKhvxndq5ODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans"
      ],
      "metadata": {
        "id": "A11kGuv54eR7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset using read.csv function of Pyspark with Header as True and inferSchema as true to check the structure of the dataset"
      ],
      "metadata": {
        "id": "6S6p01_C5Ww2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=spark.read.csv(\"/content/hack_data.csv\",header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "eHO1GT7F4mFO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting the number of Rows from the datasets"
      ],
      "metadata": {
        "id": "084CEUI_5k41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXnzH4CLnHu7",
        "outputId": "4dc0f2ef-3498-4525-b031-50b53316bea0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PrintSchema() function to check the strucure of the data"
      ],
      "metadata": {
        "id": "yVQd0mIr5pWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAjpOfUG8Ash",
        "outputId": "1b4704d9-dc94-46b6-d9b3-b8c07f377758"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Session_Connection_Time: double (nullable = true)\n",
            " |-- Bytes Transferred: double (nullable = true)\n",
            " |-- Kali_Trace_Used: integer (nullable = true)\n",
            " |-- Servers_Corrupted: double (nullable = true)\n",
            " |-- Pages_Corrupted: double (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- WPM_Typing_Speed: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the first 5 rows of the data"
      ],
      "metadata": {
        "id": "ycNeMzLe5yi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm1-MIif9kvM",
        "outputId": "3cce68e6-a7c6-47fa-fceb-964bdc82b6aa"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37),\n",
              " Row(Session_Connection_Time=20.0, Bytes Transferred=720.99, Kali_Trace_Used=0, Servers_Corrupted=3.04, Pages_Corrupted=9.0, Location='British Virgin Islands', WPM_Typing_Speed=69.08),\n",
              " Row(Session_Connection_Time=31.0, Bytes Transferred=356.32, Kali_Trace_Used=1, Servers_Corrupted=3.71, Pages_Corrupted=8.0, Location='Tokelau', WPM_Typing_Speed=70.58),\n",
              " Row(Session_Connection_Time=2.0, Bytes Transferred=228.08, Kali_Trace_Used=1, Servers_Corrupted=2.48, Pages_Corrupted=8.0, Location='Bolivia', WPM_Typing_Speed=70.8),\n",
              " Row(Session_Connection_Time=20.0, Bytes Transferred=408.5, Kali_Trace_Used=0, Servers_Corrupted=3.57, Pages_Corrupted=8.0, Location='Iraq', WPM_Typing_Speed=71.28)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Use describe().show() function to obtain the statistical summaries of the data which shows us the mean Time of the whole operation by the hackers is 30 minutes with SD being 14 minutes which means either one attack is 44 minutes and the other is 16 minutes corroborating 60 minutes as maximum time. Assuming there are 2 clusters\n",
        "The Average clusters destroyed is 5 and average pages corrupted is 11 and WPM  typing speed ona an average is 57 minutes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2OlwrArA52Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XTdRVG2QZeI",
        "outputId": "5e372ebf-81ab-4326-c762-aec9e91484f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
            "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
            "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
            "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
            "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then import VectorAssembler which is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.It can also be used for Clustering"
      ],
      "metadata": {
        "id": "0rppXJnq618m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "w5GFJPJfpfel"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking for columns before applying transformations"
      ],
      "metadata": {
        "id": "nPaiCvuF7J1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_Xp_wv6pyaL",
        "outputId": "e1bd5aae-0ca3-4579-ea53-151d3f9cdb86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Session_Connection_Time',\n",
              " 'Bytes Transferred',\n",
              " 'Kali_Trace_Used',\n",
              " 'Servers_Corrupted',\n",
              " 'Pages_Corrupted',\n",
              " 'Location',\n",
              " 'WPM_Typing_Speed']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AS CLUSTERING IS UNSUPERVISED LEARNING WE DO NOT NEED LOCATION DATA AS IT IS A LABEL. THERE ARE NO LABELS IN UNSUPERVISED LEARNING. ALSO HACKERS USUALLY USE VPNs TO HIDE THEIR LOCATION SO ITS NOT THAT USEFUL SO WE DONT CONSIDER IT."
      ],
      "metadata": {
        "id": "DqXFvj5Fp6Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns=['Session_Connection_Time',\n",
        " 'Bytes Transferred',\n",
        " 'Kali_Trace_Used',\n",
        " 'Servers_Corrupted',\n",
        " 'Pages_Corrupted',\n",
        " 'WPM_Typing_Speed']"
      ],
      "metadata": {
        "id": "Yyb6K2TgqKVy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then apply Transformations to columns after removing the locations"
      ],
      "metadata": {
        "id": "58ivFEFi7fot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler=VectorAssembler(inputCols=feature_columns,outputCol=\"features\")"
      ],
      "metadata": {
        "id": "Wv6M1JTzqdml"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMING THE DATA\n"
      ],
      "metadata": {
        "id": "WOeOwNXsqyqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data=assembler.transform(dataset)"
      ],
      "metadata": {
        "id": "tKxxMrjHq1L_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check for structure of the data to see the new structure of the dataset"
      ],
      "metadata": {
        "id": "sfErDGnz7q5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13WAXr6IraXB",
        "outputId": "2f7e7ebd-b428-4572-f5bb-2462537529dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Session_Connection_Time: double (nullable = true)\n",
            " |-- Bytes Transferred: double (nullable = true)\n",
            " |-- Kali_Trace_Used: integer (nullable = true)\n",
            " |-- Servers_Corrupted: double (nullable = true)\n",
            " |-- Pages_Corrupted: double (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- WPM_Typing_Speed: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import StandardScaler to standardize the data"
      ],
      "metadata": {
        "id": "R01_Or59742Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ],
      "metadata": {
        "id": "MtC6_nl9rlEQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We standardize the features and name the new features as featuresScaled"
      ],
      "metadata": {
        "id": "T3l669d37_yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale=StandardScaler(inputCol=\"features\",outputCol=\"featuresScaled\")"
      ],
      "metadata": {
        "id": "h89v1gHort4D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then fit the scaled data and apply transformation as store it as a cluster_final_data "
      ],
      "metadata": {
        "id": "_jW4Tyy_8IZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_model=scale.fit(final_data)\n",
        "cluster_final_data=scaled_model.transform(final_data)"
      ],
      "metadata": {
        "id": "iHUdJPFssFCs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We print the schema of cluster_final_data"
      ],
      "metadata": {
        "id": "zlzjz-vk8Uw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_final_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiAV3mRotGAt",
        "outputId": "12769e24-ce0b-4ddc-ada3-d6aab6f3fac2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Session_Connection_Time: double (nullable = true)\n",
            " |-- Bytes Transferred: double (nullable = true)\n",
            " |-- Kali_Trace_Used: integer (nullable = true)\n",
            " |-- Servers_Corrupted: double (nullable = true)\n",
            " |-- Pages_Corrupted: double (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- WPM_Typing_Speed: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- featuresScaled: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the cost of clusters for different values of K so that optimum value should be selected"
      ],
      "metadata": {
        "id": "vOkrcCuY8aGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the cost for K=2 below\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rOm_6CCF8pY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol('featuresScaled')\n",
        "model = kmeans.fit(cluster_final_data)\n",
        "cost_cluster = model.summary.trainingCost\n",
        "print(\"The cost of clusters are = \" + str(cost_cluster))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmAKyHvYxs8A",
        "outputId": "9397de32-35c6-43a1-94f3-f8289ab9254a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost of clusters are = 601.7707512676691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the cost for K=3 below\n"
      ],
      "metadata": {
        "id": "1AEhIDHR82B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans().setK(3).setSeed(1).setFeaturesCol('featuresScaled')\n",
        "model = kmeans.fit(cluster_final_data)\n",
        "cost_cluster = model.summary.trainingCost\n",
        "print(\"The cost of clusters are = \" + str(cost_cluster))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltW2B1B-2Rw_",
        "outputId": "6a089d63-843e-42df-a396-ed159dc6f9ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost of clusters are = 434.75507308487596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a variable Kmeans3 and Apply KMeans function to scaled features and take K as 3 "
      ],
      "metadata": {
        "id": "tgZlJCQ69Vyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans3= KMeans(featuresCol=\"featuresScaled\",k=3)"
      ],
      "metadata": {
        "id": "v2KC8RHw-0u2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit Kmeans3 to final data and this our model for K=3"
      ],
      "metadata": {
        "id": "Yyw53AVe9nIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_k3=kmeans3.fit(cluster_final_data)"
      ],
      "metadata": {
        "id": "ttIwPfzc_o3J"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then see the prediction for K=3 and find that it is not evenly traded off as per the domain experts information provided to us "
      ],
      "metadata": {
        "id": "mzRL_Xni9ubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_k3.transform(cluster_final_data).groupBy(\"prediction\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcrFPFnA_w6U",
        "outputId": "0b1006f8-bb1c-445a-ecba-ca5fda60966c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         1|   79|\n",
            "|         2|   88|\n",
            "|         0|  167|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a model for K=2 and find that its evely split so we conclude that there are 2 hackers"
      ],
      "metadata": {
        "id": "rJ7E5yas-IzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans2= KMeans(featuresCol=\"featuresScaled\",k=2)"
      ],
      "metadata": {
        "id": "6N3z8mHG_477"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_k2=kmeans2.fit(cluster_final_data)"
      ],
      "metadata": {
        "id": "ZIYnpXqlBCsi"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_k2.transform(cluster_final_data).groupBy(\"prediction\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqNxzUJ4BH4z",
        "outputId": "7885c94e-1496-403e-c9ed-0ed3c9891855"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         1|  167|\n",
            "|         0|  167|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}